TP_LoRA(
  (swin_backbone): SwinTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
              (tp_lora_q): TP_LoRA_Adapter(
                (adapter_down): Linear(in_features=96, out_features=8, bias=False)
                (adapter_up): Linear(in_features=8, out_features=96, bias=False)
                (act): ReLU()
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
                (mlp): Sequential(
                  (0): Linear(in_features=768, out_features=2, bias=True)
                  (1): ReLU()
                  (2): Linear(in_features=2, out_features=96, bias=True)
                )
              )
              (tp_lora_v): TP_LoRA_Adapter(
                (adapter_down): Linear(in_features=96, out_features=8, bias=False)
                (adapter_up): Linear(in_features=8, out_features=96, bias=False)
                (act): ReLU()
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
                (mlp): Sequential(
                  (0): Linear(in_features=768, out_features=2, bias=True)
                  (1): ReLU()
                  (2): Linear(in_features=2, out_features=96, bias=True)
                )
              )
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (tp_lora_mlp): TP_LoRA_Adapter(
              (adapter_down): Linear(in_features=96, out_features=8, bias=False)
              (adapter_up): Linear(in_features=8, out_features=96, bias=False)
              (act): ReLU()
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (mlp): Sequential(
                (0): Linear(in_features=768, out_features=2, bias=True)
                (1): ReLU()
                (2): Linear(in_features=2, out_features=96, bias=True)
              )
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
              (tp_lora_q): TP_LoRA_Adapter(
                (adapter_down): Linear(in_features=96, out_features=8, bias=False)
                (adapter_up): Linear(in_features=8, out_features=96, bias=False)
                (act): ReLU()
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
                (mlp): Sequential(
                  (0): Linear(in_features=768, out_features=2, bias=True)
                  (1): ReLU()
                  (2): Linear(in_features=2, out_features=96, bias=True)
                )
              )
              (tp_lora_v): TP_LoRA_Adapter(
                (adapter_down): Linear(in_features=96, out_features=8, bias=False)
                (adapter_up): Linear(in_features=8, out_features=96, bias=False)
                (act): ReLU()
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
                (mlp): Sequential(
                  (0): Linear(in_features=768, out_features=2, bias=True)
                  (1): ReLU()
                  (2): Linear(in_features=2, out_features=96, bias=True)
                )
              )
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (tp_lora_mlp): TP_LoRA_Adapter(
              (adapter_down): Linear(in_features=96, out_features=8, bias=False)
              (adapter_up): Linear(in_features=8, out_features=96, bias=False)
              (act): ReLU()
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (mlp): Sequential(
                (0): Linear(in_features=768, out_features=2, bias=True)
                (1): ReLU()
                (2): Linear(in_features=2, out_features=96, bias=True)
              )
            )
          )
        )
        (downsample): PatchMerging(
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BasicLayer(
        (blocks): ModuleList(
          (0-1): 2 x SwinTransformerBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
              (tp_lora_q): TP_LoRA_Adapter(
                (adapter_down): Linear(in_features=192, out_features=8, bias=False)
                (adapter_up): Linear(in_features=8, out_features=192, bias=False)
                (act): ReLU()
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
                (mlp): Sequential(
                  (0): Linear(in_features=768, out_features=2, bias=True)
                  (1): ReLU()
                  (2): Linear(in_features=2, out_features=192, bias=True)
                )
              )
              (tp_lora_v): TP_LoRA_Adapter(
                (adapter_down): Linear(in_features=192, out_features=8, bias=False)
                (adapter_up): Linear(in_features=8, out_features=192, bias=False)
                (act): ReLU()
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
                (mlp): Sequential(
                  (0): Linear(in_features=768, out_features=2, bias=True)
                  (1): ReLU()
                  (2): Linear(in_features=2, out_features=192, bias=True)
                )
              )
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (tp_lora_mlp): TP_LoRA_Adapter(
              (adapter_down): Linear(in_features=192, out_features=8, bias=False)
              (adapter_up): Linear(in_features=8, out_features=192, bias=False)
              (act): ReLU()
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (mlp): Sequential(
                (0): Linear(in_features=768, out_features=2, bias=True)
                (1): ReLU()
                (2): Linear(in_features=2, out_features=192, bias=True)
              )
            )
          )
        )
        (downsample): PatchMerging(
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        (blocks): ModuleList(
          (0-5): 6 x SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
              (tp_lora_q): TP_LoRA_Adapter(
                (adapter_down): Linear(in_features=384, out_features=8, bias=False)
                (adapter_up): Linear(in_features=8, out_features=384, bias=False)
                (act): ReLU()
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
                (mlp): Sequential(
                  (0): Linear(in_features=768, out_features=2, bias=True)
                  (1): ReLU()
                  (2): Linear(in_features=2, out_features=384, bias=True)
                )
              )
              (tp_lora_v): TP_LoRA_Adapter(
                (adapter_down): Linear(in_features=384, out_features=8, bias=False)
                (adapter_up): Linear(in_features=8, out_features=384, bias=False)
                (act): ReLU()
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
                (mlp): Sequential(
                  (0): Linear(in_features=768, out_features=2, bias=True)
                  (1): ReLU()
                  (2): Linear(in_features=2, out_features=384, bias=True)
                )
              )
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (tp_lora_mlp): TP_LoRA_Adapter(
              (adapter_down): Linear(in_features=384, out_features=8, bias=False)
              (adapter_up): Linear(in_features=8, out_features=384, bias=False)
              (act): ReLU()
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (mlp): Sequential(
                (0): Linear(in_features=768, out_features=2, bias=True)
                (1): ReLU()
                (2): Linear(in_features=2, out_features=384, bias=True)
              )
            )
          )
        )
        (downsample): PatchMerging(
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        (blocks): ModuleList(
          (0-1): 2 x SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
              (tp_lora_q): TP_LoRA_Adapter(
                (adapter_down): Linear(in_features=768, out_features=8, bias=False)
                (adapter_up): Linear(in_features=8, out_features=768, bias=False)
                (act): ReLU()
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
                (mlp): Sequential(
                  (0): Linear(in_features=768, out_features=2, bias=True)
                  (1): ReLU()
                  (2): Linear(in_features=2, out_features=768, bias=True)
                )
              )
              (tp_lora_v): TP_LoRA_Adapter(
                (adapter_down): Linear(in_features=768, out_features=8, bias=False)
                (adapter_up): Linear(in_features=8, out_features=768, bias=False)
                (act): ReLU()
                (dropout1): Dropout(p=0.1, inplace=False)
                (dropout2): Dropout(p=0.1, inplace=False)
                (mlp): Sequential(
                  (0): Linear(in_features=768, out_features=2, bias=True)
                  (1): ReLU()
                  (2): Linear(in_features=2, out_features=768, bias=True)
                )
              )
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (tp_lora_mlp): TP_LoRA_Adapter(
              (adapter_down): Linear(in_features=768, out_features=8, bias=False)
              (adapter_up): Linear(in_features=8, out_features=768, bias=False)
              (act): ReLU()
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (mlp): Sequential(
                (0): Linear(in_features=768, out_features=2, bias=True)
                (1): ReLU()
                (2): Linear(in_features=2, out_features=768, bias=True)
              )
            )
          )
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (avgpool): Sequential()
    (head): Sequential()
  )
  (convert1): Conv2d(512, 192, kernel_size=(1, 1), stride=(1, 1))
  (convert2): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1))
  (convert3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
  (convert4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
  (Att4): Attention_block(
    (W_g): Sequential(
      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (W_x): Sequential(
      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (psi): Sequential(
      (0): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Sigmoid()
    )
    (tp_lora_adapter_cnn): TP_LoRA_Adapter_CNN(
      (adapter_down): Linear(in_features=512, out_features=8, bias=False)
      (adapter_up): Linear(in_features=8, out_features=512, bias=False)
      (act): ReLU()
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=768, out_features=2, bias=True)
        (1): ReLU()
        (2): Linear(in_features=2, out_features=512, bias=True)
      )
    )
    (relu): ReLU(inplace=True)
  )
  (Att3): Attention_block(
    (W_g): Sequential(
      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (W_x): Sequential(
      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (psi): Sequential(
      (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Sigmoid()
    )
    (tp_lora_adapter_cnn): TP_LoRA_Adapter_CNN(
      (adapter_down): Linear(in_features=256, out_features=8, bias=False)
      (adapter_up): Linear(in_features=8, out_features=256, bias=False)
      (act): ReLU()
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=768, out_features=2, bias=True)
        (1): ReLU()
        (2): Linear(in_features=2, out_features=256, bias=True)
      )
    )
    (relu): ReLU(inplace=True)
  )
  (Att2): Attention_block(
    (W_g): Sequential(
      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (W_x): Sequential(
      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (psi): Sequential(
      (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Sigmoid()
    )
    (tp_lora_adapter_cnn): TP_LoRA_Adapter_CNN(
      (adapter_down): Linear(in_features=128, out_features=8, bias=False)
      (adapter_up): Linear(in_features=8, out_features=128, bias=False)
      (act): ReLU()
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=768, out_features=2, bias=True)
        (1): ReLU()
        (2): Linear(in_features=2, out_features=128, bias=True)
      )
    )
    (relu): ReLU(inplace=True)
  )
  (Att1): Attention_block(
    (W_g): Sequential(
      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (W_x): Sequential(
      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (psi): Sequential(
      (0): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Sigmoid()
    )
    (tp_lora_adapter_cnn): TP_LoRA_Adapter_CNN(
      (adapter_down): Linear(in_features=64, out_features=8, bias=False)
      (adapter_up): Linear(in_features=8, out_features=64, bias=False)
      (act): ReLU()
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=768, out_features=2, bias=True)
        (1): ReLU()
        (2): Linear(in_features=2, out_features=64, bias=True)
      )
    )
    (relu): ReLU(inplace=True)
  )
  (bottle): Sequential(
    (0): Conv2d(384, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (up4): up_conv(
    (up): Sequential(
      (0): Upsample(scale_factor=2.0, mode='nearest')
      (1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU(inplace=True)
    )
  )
  (up3): up_conv(
    (up): Sequential(
      (0): Upsample(scale_factor=2.0, mode='nearest')
      (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU(inplace=True)
    )
  )
  (up2): up_conv(
    (up): Sequential(
      (0): Upsample(scale_factor=2.0, mode='nearest')
      (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU(inplace=True)
    )
  )
  (up1): up_conv(
    (up): Sequential(
      (0): Upsample(scale_factor=2.0, mode='nearest')
      (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): ReLU(inplace=True)
    )
  )
  (conv4): conv_block(
    (conv): Sequential(
      (0): Conv2d(704, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (conv3): conv_block(
    (conv): Sequential(
      (0): Conv2d(352, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (conv2): conv_block(
    (conv): Sequential(
      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (conv1): conv_block(
    (conv): Sequential(
      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (final_conv): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))
  (cnn1): conv_block(
    (conv): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (cnn2): conv_block(
    (conv): Sequential(
      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (downSample): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (change1): Sequential(
    (0): Conv2d(192, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (change2): Sequential(
    (0): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (tp_lora_adapter1): TP_LoRA_Adapter_CNN(
    (adapter_down): Linear(in_features=64, out_features=8, bias=False)
    (adapter_up): Linear(in_features=8, out_features=64, bias=False)
    (act): ReLU()
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=768, out_features=2, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2, out_features=64, bias=True)
    )
  )
  (tp_lora_adapter2): TP_LoRA_Adapter_CNN(
    (adapter_down): Linear(in_features=128, out_features=8, bias=False)
    (adapter_up): Linear(in_features=8, out_features=128, bias=False)
    (act): ReLU()
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=768, out_features=2, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2, out_features=128, bias=True)
    )
  )
  (tp_lora_adapter3): TP_LoRA_Adapter_CNN(
    (adapter_down): Linear(in_features=256, out_features=8, bias=False)
    (adapter_up): Linear(in_features=8, out_features=256, bias=False)
    (act): ReLU()
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=768, out_features=2, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2, out_features=256, bias=True)
    )
  )
  (tp_lora_adapter4): TP_LoRA_Adapter_CNN(
    (adapter_down): Linear(in_features=512, out_features=8, bias=False)
    (adapter_up): Linear(in_features=8, out_features=512, bias=False)
    (act): ReLU()
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=768, out_features=2, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2, out_features=512, bias=True)
    )
  )
)
